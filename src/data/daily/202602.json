[
  {
    "date": "2026-02-05",
    "projects": [
      {
        "name": "Mosaic Learning: A Framework for Decentralized Learning with Model Fragmentation",
        "url": "https://arxiv.org/abs/2602.04352",
        "type": "论文",
        "tags": [
          "Sayan Biswas",
          "Fragmentation",
          "Non-IID",
          "Gossip"
        ],
        "summary": {
          "zh": "Biswas等人在{{name}}论文中提出Mosaic Learning框架，通过将模型参数分片并在不同通信图中独立传播，提升去中心化学习在非IID数据下的节点级性能，并给出理论与实验验证。",
          "en": "In the {{name}}paper, Biswas et al. introduced Mosaic Learning, a decentralized learning framework that uses model fragmentation across multiple gossip graphs to improve node-level performance, especially under non-IID data."
        },
        "slogan": {
          "zh": "【ML论文】模型不再整包发：Mosaic Learning 重塑去中心化训练"
        },
        "notes": {
          "zh": [
            "提出将model fragmentation作为去中心化学习的一等设计维度，而非系统层技巧",
            "不同模型分片通过独立gossip图传播，增强信息多样性并减少冗余同步",
            "理论证明最坏情况下收敛率不劣于Epidemic Learning（EL）",
            "凸问题中分片数量增加可降低系统收缩因子，加快一致性",
            "实验显示在强non-IID场景下，节点平均性能最高提升约12%",
            "发现consensus distance在非凸任务中不再是可靠的性能指标"
          ],
          "en": [
            "Treats model fragmentation as a first-class learning primitive in decentralized ML.",
            "Uses independent gossip graphs per fragment to improve information diversity.",
            "Matches state-of-the-art convergence guarantees of Epidemic Learning in theory.",
            "Shows faster consensus in convex settings as fragment count increases.",
            "Empirically boosts node-level accuracy under strong non-IID data.",
            "Finds consensus distance unreliable for non-convex performance evaluation."
          ]
        }
      }
    ],
    "qas": [
      {
        "level": "beginner",
        "question": {
          "zh": "Mosaic Learning 和传统的去中心化学习有什么直观区别？",
          "en": "How is Mosaic Learning intuitively different from standard decentralized learning?"
        },
        "answer": {
          "zh": "传统方法每轮交换完整模型，而 Mosaic Learning 将模型拆成多个子空间，每个子空间走不同的通信路径。这类似把一次“整体验证”拆成多次“分片验证”，让信息传播更分散、更高效。",
          "en": "Standard DL exchanges full models each round, while Mosaic Learning sends different parameter fragments over different paths. This spreads information more evenly, similar to splitting a monolithic check into multiple independent checks."
        }
      },
      {
        "level": "intermediate",
        "question": {
          "zh": "为什么模型分片不会破坏去中心化学习的收敛性？",
          "en": "Why does model fragmentation not hurt convergence guarantees?"
        },
        "answer": {
          "zh": "论文证明最坏情况下，每个分片的 gossip 在期望上等价于原始 EL 的通信算子。分片后分别分析再合并，可得到与 EL 相同的收敛率，且与分片数 K 无关，类似 batch evaluation 在总体上保持复杂度不变。",
          "en": "Each fragment's gossip operator matches Epidemic Learning in expectation. Analyzing fragments separately and aggregating results yields the same worst-case convergence rate as EL, independent of K, akin to batch evaluation preserving asymptotic cost."
        }
      },
      {
        "level": "expert",
        "question": {
          "zh": "从线性系统角度看，为什么增加分片数 K 会加快一致性？",
          "en": "From a linear system view, why does increasing K speed up consensus?"
        },
        "answer": {
          "zh": "分片使 gossip 矩阵呈 block-diagonal 结构，不同参数子空间独立混合。在线性二次模型中，这会降低系统收缩矩阵的最大特征值，减少共识误差，效果类似 sum-check 中分解高维多项式以加快验证。",
          "en": "Fragmentation yields a block-diagonal gossip structure where parameter subspaces mix independently. In quadratic models, this lowers the largest eigenvalue of the contraction matrix, reducing consensus error, similar to decomposing sums in sum-check protocols."
        }
      }
    ]
  }
]