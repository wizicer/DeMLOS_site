[
  {
    "date": "2026-02-05",
    "projects": [
      {
        "name": "Mosaic Learning: A Framework for Decentralized Learning with Model Fragmentation",
        "url": "https://arxiv.org/abs/2602.04352",
        "type": "论文",
        "tags": [
          "Sayan Biswas",
          "Fragmentation",
          "Non-IID",
          "Gossip"
        ],
        "summary": {
          "zh": "Biswas等人在{{name}}论文中提出Mosaic Learning框架，通过将模型参数分片并在不同通信图中独立传播，提升去中心化学习在非IID数据下的节点级性能，并给出理论与实验验证。",
          "en": "In the {{name}}paper, Biswas et al. introduced Mosaic Learning, a decentralized learning framework that uses model fragmentation across multiple gossip graphs to improve node-level performance, especially under non-IID data."
        },
        "slogan": {
          "zh": "【ML论文】模型不再整包发：Mosaic Learning 重塑去中心化训练"
        },
        "notes": {
          "zh": [
            "提出将model fragmentation作为去中心化学习的一等设计维度，而非系统层技巧",
            "不同模型分片通过独立gossip图传播，增强信息多样性并减少冗余同步",
            "理论证明最坏情况下收敛率不劣于Epidemic Learning（EL）",
            "凸问题中分片数量增加可降低系统收缩因子，加快一致性",
            "实验显示在强non-IID场景下，节点平均性能最高提升约12%",
            "发现consensus distance在非凸任务中不再是可靠的性能指标"
          ],
          "en": [
            "Treats model fragmentation as a first-class learning primitive in decentralized ML.",
            "Uses independent gossip graphs per fragment to improve information diversity.",
            "Matches state-of-the-art convergence guarantees of Epidemic Learning in theory.",
            "Shows faster consensus in convex settings as fragment count increases.",
            "Empirically boosts node-level accuracy under strong non-IID data.",
            "Finds consensus distance unreliable for non-convex performance evaluation."
          ]
        }
      }
    ],
    "qas": [
      {
        "level": "beginner",
        "question": {
          "zh": "Mosaic Learning 和传统的去中心化学习有什么直观区别？",
          "en": "How is Mosaic Learning intuitively different from standard decentralized learning?"
        },
        "answer": {
          "zh": "传统方法每轮交换完整模型，而 Mosaic Learning 将模型拆成多个子空间，每个子空间走不同的通信路径。这类似把一次“整体验证”拆成多次“分片验证”，让信息传播更分散、更高效。",
          "en": "Standard DL exchanges full models each round, while Mosaic Learning sends different parameter fragments over different paths. This spreads information more evenly, similar to splitting a monolithic check into multiple independent checks."
        }
      },
      {
        "level": "intermediate",
        "question": {
          "zh": "为什么模型分片不会破坏去中心化学习的收敛性？",
          "en": "Why does model fragmentation not hurt convergence guarantees?"
        },
        "answer": {
          "zh": "论文证明最坏情况下，每个分片的 gossip 在期望上等价于原始 EL 的通信算子。分片后分别分析再合并，可得到与 EL 相同的收敛率，且与分片数 K 无关，类似 batch evaluation 在总体上保持复杂度不变。",
          "en": "Each fragment's gossip operator matches Epidemic Learning in expectation. Analyzing fragments separately and aggregating results yields the same worst-case convergence rate as EL, independent of K, akin to batch evaluation preserving asymptotic cost."
        }
      },
      {
        "level": "expert",
        "question": {
          "zh": "从线性系统角度看，为什么增加分片数 K 会加快一致性？",
          "en": "From a linear system view, why does increasing K speed up consensus?"
        },
        "answer": {
          "zh": "分片使 gossip 矩阵呈 block-diagonal 结构，不同参数子空间独立混合。在线性二次模型中，这会降低系统收缩矩阵的最大特征值，减少共识误差，效果类似 sum-check 中分解高维多项式以加快验证。",
          "en": "Fragmentation yields a block-diagonal gossip structure where parameter subspaces mix independently. In quadratic models, this lowers the largest eigenvalue of the contraction matrix, reducing consensus error, similar to decomposing sums in sum-check protocols."
        }
      }
    ]
  },
  {
    "date": "2026-02-07",
    "projects": [
      {
        "name": "Privacy-Preserving LLM Inference in Practice: A Comparative Survey of Techniques, Trade-Offs, and Deployability",
        "url": "https://eprint.iacr.org/2026/105",
        "type": "论文",
        "tags": [
          "Davide Andreoletti",
          "Confidential Inference"
        ],
        "summary": {
          "zh": "Andreoletti等人在{{name}}论文中系统梳理了隐私保护LLM推理技术，从TEE、密码增强方案到FHE，对比其信任假设、可扩展性与部署成熟度，强调端到端仅客户端可见的强隐私模型。",
          "en": "In the {{name}}paper, Andreoletti et al. survey privacy-preserving LLM inference techniques, comparing TEE, crypto-augmented designs, and FHE in terms of trust, scalability, and deployment readiness under a strong end-to-end privacy model."
        },
        "slogan": {
          "zh": "【ZK/隐私计算】隐私版LLM如何落地？从TEE走向FHE的现实路线图"
        },
        "notes": {
          "zh": [
            "提出强隐私定义：Prompt与生成结果端到端仅客户端可见",
            "系统分析PETs在LLM推理中的应用，覆盖非线性层与自回归解码瓶颈",
            "TEE方案当前最具可部署性，但依赖硬件信任假设",
            "密码增强方案可降低对TEE的信任依赖，但计算成本显著上升",
            "FHE被视为长期理想方案，支持非交互式机密推理但尚不实用",
            "论文强调现实部署需在安全性、性能与信任最小化之间权衡"
          ],
          "en": [
            "Defines a strong privacy model where only clients can access prompts and outputs.",
            "Reviews PET-based approaches for LLM inference, including handling non-linear layers and autoregression.",
            "TEE-based systems are the most deployable today but rely on hardware trust.",
            "Crypto-augmented designs reduce hardware trust at higher computational cost.",
            "FHE is positioned as a principled long-term solution for confidential inference.",
            "Highlights trade-offs between security, performance, and deployability."
          ]
        }
      }
    ],
    "qas": [
      {
        "level": "beginner",
        "variant": 2,
        "question": {
          "zh": "为什么 Transformer 的非线性层对隐私推理特别困难？",
          "en": "Why are non-linear layers in Transformers especially challenging for private inference?"
        },
        "answer": {
          "zh": "非线性函数如 GELU、Softmax 难以在加密或受限执行环境中高效实现。它们往往需要近似、多轮交互或额外信任假设，成为隐私推理系统的主要性能瓶颈。",
          "en": "Non-linear functions such as GELU and Softmax are hard to implement efficiently under encryption or restricted execution. They often require approximations, interaction, or extra trust assumptions, making them major performance bottlenecks."
        }
      },
      {
        "level": "intermediate",
        "variant": 1,
        "question": {
          "zh": "TEE 方案为何被认为是当前最可部署的隐私推理路径？",
          "en": "Why are TEE-based solutions considered the most deployable today?"
        },
        "answer": {
          "zh": "TEE 能在几乎原生性能下运行完整模型，并支持自回归解码，工程复杂度较低。其主要代价是对硬件厂商和侧信道防护的信任，而非计算成本。",
          "en": "TEEs can run full models at near-native performance and support autoregressive decoding with low engineering complexity. The main cost is reliance on hardware trust and side-channel defenses rather than computation."
        }
      },
      {
        "level": "expert",
        "variant": 1,
        "question": {
          "zh": "自回归解码为何成为区分隐私推理方案可行性的关键因素？",
          "en": "Why is autoregressive decoding a key differentiator for private inference systems?"
        },
        "answer": {
          "zh": "自回归解码需要逐 token 依赖前序输出，这在交互式或高延迟加密方案中极其昂贵。能否高效支持解码，直接决定系统是否适用于真实 LLM 服务。",
          "en": "Autoregressive decoding requires token-by-token dependency, which is extremely costly under interactive or high-latency cryptographic schemes. Efficient decoding support largely determines real-world applicability."
        }
      }
    ]
  },
  {
    "date": "2026-02-08",
    "projects": [
      {
        "url": "https://arxiv.org/abs/2601.10823",
        "name": "Mugi: Value Level Parallelism For Efficient LLMs",
        "type": "论文",
        "tags": [
          "Daniel Price",
          "VLP",
          "GEMM"
        ],
        "summary": {
          "zh": "Price等人在{{name}}论文中系统研究了VLP在LLM中的应用，提出面向非线性与小批量场景的VLP优化，并设计新架构Mugi，在吞吐、能效与可持续性上显著优于现有方案。",
          "en": "In the {{name}}paper, Price et al. study VLP for LLMs, extending it to nonlinear ops and small-batch GEMMs, and propose the Mugi architecture to improve throughput, energy efficiency, and sustainability."
        },
        "slogan": {
          "zh": "【LLM系统】不只是GEMM：Mugi用VLP重塑高效与低碳的LLM推理"
        },
        "notes": {
          "zh": [
            "将VLP从对称GEMM推广到LLM中的非线性近似计算",
            "采用以值为中心的近似策略，为重要数值分配更高精度",
            "在端到端LLM精度与性能上优于现有非线性近似方法",
            "针对小batch、非对称输入的GEMM优化VLP实现",
            "结合权重量化、KV Cache量化与GQA等主流LLM优化",
            "Mugi架构在吞吐、能耗及运行与制造碳排放上均显著下降"
          ],
          "en": [
            "Extends VLP beyond symmetric GEMM to nonlinear LLM operations.",
            "Uses value-centric approximations to preserve important values.",
            "Achieves better end-to-end LLM accuracy and performance.",
            "Optimizes VLP for small-batch and asymmetric GEMMs.",
            "Integrates weight-only and KV cache quantization with GQA.",
            "Mugi improves throughput, energy efficiency, and reduces carbon footprint."
          ]
        }
      }
    ],
    "qas": [
      {
        "level": "beginner",
        "question": {
          "zh": "什么是 Value Level Parallelism（VLP），它最初解决什么问题？",
          "en": "What is Value Level Parallelism (VLP), and what problem was it originally designed to solve?"
        },
        "answer": {
          "zh": "VLP 是一种利用数值分布特性的并行计算方式，最初用于加速低精度、大批量的 GEMM 运算。它通过对不同“数值重要性”采用不同精度或计算路径，提高吞吐量与能效。",
          "en": "VLP is a parallelization technique that exploits value distributions. It was originally proposed to accelerate low-precision, large-batch GEMMs by assigning different accuracy or compute paths to values of different importance."
        }
      },
      {
        "level": "intermediate",
        "question": {
          "zh": "为什么小 batch、非对称 GEMM 对 VLP 是一项挑战？",
          "en": "Why are small-batch, asymmetric GEMMs challenging for VLP?"
        },
        "answer": {
          "zh": "传统 VLP 假设对称输入和大批量以摊销开销。LLM 推理中常见的小 batch、权重量化与 KV cache 量化破坏了这些假设，需要重新设计调度与数据路径。",
          "en": "Classic VLP assumes symmetric inputs and large batches to amortize overhead. LLM inference often uses small batches with weight-only and KV-cache quantization, breaking these assumptions and requiring new designs."
        }
      },
      {
        "level": "expert",
        "question": {
          "zh": "Mugi 架构如何同时支持多种 LLM 优化而不牺牲通用性？",
          "en": "How does the Mugi architecture support multiple LLM optimizations without sacrificing generality?"
        },
        "answer": {
          "zh": "Mugi 将 VLP 抽象为统一的值级执行框架，使权重量化、KV cache 量化和 GQA 成为可组合策略，而非硬编码路径，从而覆盖完整 Transformer 工作负载。",
          "en": "Mugi abstracts VLP into a unified value-level execution framework, making weight quantization, KV-cache quantization, and GQA composable strategies rather than fixed paths, enabling full Transformer support."
        }
      }
    ]
  },
  {
    "date": "2026-02-10",
    "projects": [
      {
        "name": "InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation",
        "url": "https://arxiv.org/abs/2602.08229",
        "type": "论文",
        "tags": [
          "Yifan Yang",
          "LLM Evaluation"
        ],
        "summary": {
          "zh": "作者在{{name}}论文中提出了一种基于区块链的去中心化LLM评估框架，通过激励全球贡献者作为独立验证者，利用异构计算节点进行大规模基准测试，以解决集中式评估的不透明、过拟合和硬件差异问题。",
          "en": "The authors propose a blockchain-based decentralized LLM evaluation framework in their {{name}} paper. It incentivizes global contributors as independent validators for large-scale benchmarking across heterogeneous compute nodes, addressing issues like opacity and hardware variance in centralized evaluations."
        },
        "notes": {
          "zh": [
            "集中式LLM评估不一致：HumanEval标准差(1.67)超过前10模型差距(0.91)。",
            "核心问题：评估不透明、模型过拟合、硬件差异导致波动。",
            "去中心化框架利用区块链激励全球参与者成为独立验证者。",
            "通过异构计算节点进行大规模基准测试，引入多样性。",
            "稳健奖励系统确保评估完整性并阻止不诚实参与。",
            "实验将标准差降至0.28，显著提升模型排名置信度。"
          ],
          "en": [
            "Centralized LLM evaluation shows high inconsistency: HumanEval std dev (1.67) exceeds top-10 model gap (0.91).",
            "Issues: opacity, overfitting, hardware-induced variance.",
            "Decentralized blockchain framework incentivizes global validators.",
            "Enables diversity via heterogeneous compute nodes.",
            "Robust reward system ensures integrity.",
            "Reduces std dev to 0.28, improving ranking confidence."
          ]
        }
      }
    ],
    "qas": [
      {
        "level": "beginner",
        "question": {
          "zh": "InfiCoEvalChain框架是如何利用区块链技术的？",
          "en": "How does the InfiCoEvalChain framework utilize blockchain technology?"
        },
        "answer": {
          "zh": "InfiCoEvalChain利用区块链激励全球参与者贡献计算资源成为独立验证节点，运行LLM评估任务。区块链确保评估记录不可篡改、透明，并通过共识机制保证结果完整性。",
          "en": "InfiCoEvalChain uses blockchain to incentivize global participants to contribute computing resources as independent validators for LLM evaluations. Blockchain ensures tamper-proof, transparent records and uses consensus to guarantee result integrity."
        }
      },
      {
        "level": "intermediate",
        "question": {
          "zh": "InfiCoEvalChain的\"去中心化背书\"机制相比传统评估有哪些优势？",
          "en": "What advantages does InfiCoEvalChain's decentralized endorsement offer over traditional evaluation?"
        },
        "answer": {
          "zh": "核心优势包括：1. **硬件多样性**：在全球异构节点上评估，减少单一环境偏差；2. **多方共识**：结果需多个独立验证者确认，提高可信度；3. **激励透明**：通过公开奖励系统激励诚实评估。",
          "en": "Key advantages: 1. **Hardware Diversity**: Evaluations on global heterogeneous nodes reduce single-environment bias; 2. **Multi-party Consensus**: Results require multiple independent validators, enhancing credibility; 3. **Transparent Incentives**: Public reward system incentivizes honest evaluation."
        }
      },
      {
        "level": "expert",
        "question": {
          "zh": "InfiCoEvalChain如何抵御女巫攻击或验证者合谋？",
          "en": "How does InfiCoEvalChain resist Sybil Attacks or validator collusion?"
        },
        "answer": {
          "zh": "对策包括：1. **女巫攻击**：结合质押机制或可信硬件验证身份；2. **验证者合谋**：随机分配任务、盲评估、Slashing惩罚使合谋收益为负；3. **奖励设计**：引入信誉系统区分诚实与恶意行为。",
          "en": "Countermeasures: 1. **Sybil Attacks**: Staking mechanisms or trusted hardware for identity; 2. **Collusion**: Random task assignment, blind evaluation, slashing penalties; 3. **Incentive Design**: Reputation systems to distinguish honest from malicious behavior."
        }
      }
    ]
  },
  {
    "date": "2026-02-11",
    "projects": [
      {
        "url": "https://arxiv.org/abs/2602.09109",
        "name": "Distributed Hybrid Parallelism for Large Language Models: Comparative Study and System Design Guide",
        "type": "论文",
        "tags": [
          "Hossam Amer",
          "Parallelism"
        ],
        "summary": {
          "zh": "该论文对LLM分布式训练与推理中的并行策略进行了全面综述，通过数学模型和案例分析，为设计最优分布式系统提供了方法论指导。",
          "en": "This paper provides a comprehensive review of parallel strategies for distributed LLM training and inference, offering methodological guidance for optimal system design through mathematical models and case studies."
        },
        "slogan": {
          "zh": "【LLM系统设计】大模型分布式并行策略全解析：从理论到实践的设计指南"
        },
        "notes": {
          "zh": [
            "系统回顾LLM分布式计算中的集体操作与并行策略，提供数学公式深化理论理解。",
            "分析混合并行化设计，强调训练与推理阶段的通信计算重叠。",
            "讨论基于成本模型自动搜索最优混合并行策略的进展。",
            "通过主流架构案例研究，揭示并行策略选择的经验见解。",
            "指出当前LLM训练范式的开放挑战与局限。",
            "为下一代大规模模型开发指明研究方向。"
          ],
          "en": [
            "Reviews collective operations and parallel strategies in distributed LLM computing with mathematical formulations.",
            "Analyzes hybrid parallelization with focus on communication-computation overlap.",
            "Discusses automated search for optimal hybrid strategies using cost models.",
            "Presents case studies with mainstream architectures for strategy selection insights.",
            "Highlights open challenges of current LLM training paradigms.",
            "Outlines directions for next-generation large-scale model development."
          ]
        }
      }
    ],
    "qas": [
      {
        "level": "beginner",
        "question": {
          "zh": "论文中提到的“混合并行化”是什么意思？",
          "en": "What does \"hybrid parallelization\" refer to in the paper?"
        },
        "answer": {
          "zh": "“混合并行化”是指结合使用多种并行技术（如数据并行、模型并行、流水线并行）来更有效地分配LLM的计算和内存负载。论文强调通过重叠通信与计算来优化性能。",
          "en": "\"Hybrid parallelization\" combines data, model, and pipeline parallelism to distribute LLM workloads efficiently, with emphasis on overlapping communication and computation."
        }
      },
      {
        "level": "intermediate",
        "question": {
          "zh": "论文指出了当前LLM训练范式的哪些主要挑战？",
          "en": "What major challenges of current LLM training paradigms does the paper highlight?"
        },
        "answer": {
          "zh": "论文指出了几个开放挑战，包括在超大规模模型下通信与计算的协调效率、内存瓶颈的突破、混合策略自动化的复杂性，以及为下一代更大模型设计可扩展且成本效益高的系统架构。",
          "en": "Challenges include communication-computation coordination at scale, memory bottlenecks, automation complexity, and designing scalable architectures for larger models."
        }
      },
      {
        "level": "expert",
        "question": {
          "zh": "基于成本模型的自动化搜索对于寻找最优混合并行策略有何理论贡献和实践限制？",
          "en": "What are the theoretical contributions and practical limitations of automated search based on cost models for finding optimal hybrid parallelization strategies?"
        },
        "answer": {
          "zh": "理论贡献在于将策略搜索问题形式化为一个在约束（如内存、设备数量）下的优化问题，允许系统探索指数级大的策略空间。实践限制包括成本模型本身的准确性（依赖于对硬件和框架行为的简化假设）、搜索过程的计算开销，以及对于动态负载或异构集群环境的适应性不足。",
          "en": "Theoretical contribution: formalizing strategy search as constrained optimization over exponential space. Practical limits: cost model accuracy, search overhead, and poor adaptability to dynamic/heterogeneous environments."
        }
      }
    ]
  },
  {
    "date": "2026-02-12",
    "projects": [
      {
        "url": "https://arxiv.org/abs/2602.08019",
        "name": "The Rise of Sparse Mixture-of-Experts: A Survey from Algorithmic Foundations to Decentralized Architectures and Vertical Domain Applications",
        "type": "论文",
        "tags": [
          "Dong Pan",
          "MoE",
          "Survey"
        ],
        "summary": {
          "zh": "该论文对稀疏混合专家（MoE）架构进行了全面综述，涵盖了从算法基础、去中心化架构到垂直领域应用的最新进展，旨在为研究者和从业者提供导航。",
          "en": "This paper provides a comprehensive survey of the Sparse Mixture-of-Experts (MoE) architecture, covering recent advancements from algorithmic foundations to decentralized architectures and vertical domain applications, serving as a resource for researchers and practitioners."
        },
        "slogan": {
          "zh": "【AI论文】MoE全景图：从算法核心到去中心化架构与应用"
        },
        "notes": {
          "zh": [
            "稀疏MoE架构通过路由网络激活专家子集，以可比的算力成本扩展模型参数量。",
            "论文系统探讨了MoE的核心组件：路由网络和专家网络。",
            "研究范围从中心化范式扩展到去中心化范式，挖掘去中心化基础设施的潜力。",
            "去中心化MoE有望实现开发的民主化，带来更大的可扩展性和成本效益。",
            "论文还识别了关键挑战和未来有前景的研究方向。",
            "据作者所知，这是目前MoE领域最全面的综述。"
          ],
          "en": [
            "Sparse MoE scales parameters by activating expert subsets via routing, maintaining compute efficiency.",
            "The survey explores core MoE components: the routing network and expert network.",
            "It extends from centralized to decentralized paradigms, unlocking the potential of decentralized infrastructure.",
            "Decentralized MoE enables democratization of development, offering greater scalability and cost-efficiency.",
            "Key challenges and promising future research directions are identified.",
            "This is claimed to be the most comprehensive review in the MoE field to date."
          ]
        }
      }
    ],
    "qas": [
      {
        "level": "beginner",
        "question": {
          "zh": "什么是稀疏混合专家（MoE）架构？它的主要优势？",
          "en": "What is the Sparse Mixture-of-Experts (MoE) architecture? What is its main advantage?"
        },
        "answer": {
          "zh": "稀疏MoE是一种包含多个专家子网络和路由网络的架构，每次推理只激活部分专家。主要优势是能以较低计算成本实现海量参数规模，提升效率和成本效益。",
          "en": "Sparse MoE is an architecture with multiple expert sub-networks and a router that activates only a subset per input. Its main advantage is enabling massive parameter counts with low computational cost per inference."
        }
      },
      {
        "level": "intermediate",
        "question": {
          "zh": "论文中提到的\"去中心化MoE范式\"具体指什么？它可能带来哪些好处？",
          "en": "What does the 'decentralized MoE paradigm' mentioned in the paper refer to? What benefits might it bring?"
        },
        "answer": {
          "zh": "去中心化MoE指将模型部署在分布式网络而非集中式集群上。好处包括：民主化开发、更大可扩展性、以及通过利用分布式资源降低成本。",
          "en": "Decentralized MoE deploys models on distributed infrastructure instead of centralized clusters. Benefits include democratized development, greater scalability, and cost reduction through distributed resources."
        }
      },
      {
        "level": "expert",
        "question": {
          "zh": "在去中心化MoE的背景下，可能会面临哪些独特的技术挑战（例如，与中心化MoE相比）？论文对此有何见解？",
          "en": "In the context of decentralized MoE, what unique technical challenges might arise (compared to centralized MoE)? What insights does the paper offer on this?"
        },
        "answer": {
          "zh": "主要挑战包括：1）网络延迟影响路由效率；2）分布式参数同步与一致性；3）安全验证与激励机制设计；4）异构节点间的负载均衡。论文指出需要ML、分布式系统和密码学的交叉创新。",
          "en": "Key challenges: 1) Network latency affecting routing; 2) Distributed parameter synchronization; 3) Security verification and incentive design; 4) Load balancing across heterogeneous nodes. The paper suggests cross-disciplinary innovation in ML, distributed systems, and cryptography."
        }
      }
    ]
  },
  {
    "date": "2026-02-19",
    "projects": [
      {
        "url": "https://arxiv.org/abs/2509.23248",
        "name": "Agentic AI Reasoning for Mobile Edge General Intelligence: Fundamentals, Approaches, and Directions",
        "type": "论文",
        "tags": [
          "Mingyi Luo",
          "Edge Computing",
          "MEGI",
          "MoE"
        ],
        "summary": {
          "zh": "Luo 等人在论文中提出了一种用于移动边缘通用智能（MEGI）的联合优化框架，旨在解决边缘设备上部署基于LLM的智能体AI推理时面临的高计算需求和资源限制挑战。",
          "en": "Luo et al. propose a joint optimization framework for Mobile Edge General Intelligence (MEGI) in their paper to address the challenges of deploying LLM-based agentic AI reasoning on resource-constrained edge devices."
        },
        "slogan": {
          "zh": "【AI论文】让大模型在手机边缘跑起来：新框架平衡推理质量与资源效率"
        },
        "notes": {
          "zh": [
            "LLM与边缘计算结合催生了MEGI，提供实时、注重隐私的边缘推理。",
            "MEGI面临LLM高算力需求与边缘设备资源有限的挑战。",
            "提出联合优化框架，结合自适应CoT推理与分布式MoE部署。",
            "核心创新：将推理深度视为动态资源，与专家激活及传输功率联合优化。",
            "系统按任务与设备能力，动态调节专家网络与推理复杂度。",
            "实验证明该框架平衡了推理质量与资源，验证了其在MEGI中的可行性。"
          ],
          "en": [
            "Integrating LLM agents with edge computing creates MEGI for real-time, private reasoning.",
            "Deploying LLMs in MEGI faces high compute demands and limited edge resources.",
            "A joint framework combines adaptive CoT reasoning with distributed MoE deployment.",
            "Innovation: Jointly optimizing reasoning depth, expert activation, and transmission power.",
            "The system dynamically adjusts expert networks and complexity per task and device.",
            "Experiments confirm the framework balances quality and efficiency, proving MEGI viability."
          ]
        }
      }
    ],
    "qas": [
      {
        "level": "beginner",
        "question": {
          "zh": "什么是移动边缘通用智能（MEGI）？它的主要目标是什么？",
          "en": "What is Mobile Edge General Intelligence (MEGI) and what is its main goal?"
        },
        "answer": {
          "zh": "MEGI是将基于LLM的智能体与边缘计算结合的系统。主要目标是将AI推理和自主决策能力部署在网络边缘，从而提供实时且保护隐私的智能服务，并减少对云端计算的依赖。",
          "en": "MEGI integrates LLM-based agentic AI with edge computing. Its main goal is to bring AI reasoning and autonomous decision-making to the network edge, enabling real-time, privacy-preserving services and reducing cloud reliance."
        }
      },
      {
        "level": "intermediate",
        "question": {
          "zh": "“将推理深度建模为动态网络资源变量”具体是什么意思？这样做有什么好处？",
          "en": "What does 'modeling reasoning depth as a dynamic network resource variable' mean, and what are the benefits?"
        },
        "answer": {
          "zh": "意为不再将LLM的推理步骤视为固定值，而是作为像带宽或算力一样可管理优化的系统资源。其好处是系统能根据任务复杂度、设备资源和网络状况，动态调整推理的精细程度（如不同复杂度的CoT），在保障推理质量的前提下最大化资源利用效率。",
          "en": "It means treating LLM reasoning steps not as fixed parameters, but as manageable system resources like bandwidth. The benefit is dynamically adjusting reasoning granularity (e.g., varying CoT complexity) based on task complexity, device resources, and network conditions, maximizing efficiency while maintaining quality."
        }
      },
      {
        "level": "expert",
        "question": {
          "zh": "该框架联合优化了哪些变量？相比于单独优化有何优势？",
          "en": "What variables are jointly optimized in this framework, and what is the advantage over separate optimization?"
        },
        "answer": {
          "zh": "该框架联合优化了三个关键耦合变量：推理深度(D)、专家网络的激活策略以及无线传输功率(P)。由于更深的推理会增加计算能耗与延迟，而传输功率影响通信可靠性，联合优化能找到全局最优的权衡点，避免单独优化时为追求低延迟而过度牺牲准确性这种次优解。",
          "en": "The framework jointly optimizes three coupled variables: reasoning depth (D), expert network activation, and wireless transmission power (P). Since deeper reasoning increases compute energy and latency, and power affects communication, joint optimization finds a globally optimal trade-off, avoiding suboptimal separate solutions like sacrificing accuracy for low latency."
        }
      }
    ]
  },
  {
    "date": "2026-02-20",
    "projects": [
      {
        "url": "https://arxiv.org/abs/2602.16814",
        "name": "Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI",
        "type": "论文",
        "tags": [
          "Eiman Kanjo",
          "Edge AI"
        ],
        "summary": {
          "zh": "该论文提出了“节点学习”这一去中心化学习范式，将智能置于单个边缘节点，通过选择性对等交互扩展知识，以解决集中式AI在异构、移动和资源受限环境中的扩展瓶颈。",
          "en": "The paper proposes 'Node Learning', a decentralized paradigm where intelligence resides at individual edge nodes and expands via selective peer interaction, addressing scaling bottlenecks of centralized AI in heterogeneous, mobile, and resource-constrained environments."
        },
        "slogan": {
          "zh": "【AI论文】告别中心化瓶颈！Node Learning 开启去中心化、自适应边缘智能新范式"
        },
        "notes": {
          "zh": [
            "集中式AI在边缘场景面临数据传输、延迟、能耗和数据中心依赖等扩展瓶颈。",
            "Node Learning 将智能置于单个边缘节点，通过选择性对等交互扩散知识。",
            "节点持续从本地数据学习，维护自身模型状态，并在有益时进行机会性知识交换。",
            "学习通过重叠和扩散传播，而非全局同步或中心化聚合。",
            "该范式在单一抽象中统一了自主和协作行为，适应数据、硬件、目标和连接性的异构性。",
            "论文探讨了该范式对通信、硬件、信任和治理的影响，为现有去中心化方法提供了更广阔的视角。"
          ],
          "en": [
            "Centralized AI faces scaling bottlenecks (data transmission, latency, energy, data center reliance) at the edge.",
            "Node Learning places intelligence at individual edge nodes, expanding knowledge via selective peer interaction.",
            "Nodes learn continuously from local data, maintain own model state, and exchange knowledge opportunistically.",
            "Learning propagates via overlap and diffusion, not global synchronization or central aggregation.",
            "The paradigm unifies autonomous/cooperative behavior and accommodates heterogeneity in data, hardware, objectives, and connectivity.",
            "The paper examines implications for communication, hardware, trust, and governance, offering a broader decentralized perspective."
          ]
        }
      }
    ],
    "qas": [
      {
        "level": "beginner",
        "question": {
          "zh": "为什么我们需要在边缘进行AI学习？Node Learning 解决了哪些问题？",
          "en": "Why do we need AI learning at the edge? What problems does Node Learning solve?"
        },
        "answer": {
          "zh": "云端处理海量物联网数据易导致高延迟和网络拥堵。Node Learning 直接在边缘设备学习，解决了传输瓶颈，实现低延迟、本地数据隐私保护以及弱网环境下的鲁棒性。",
          "en": "Processing massive IoT data in the cloud causes high latency and network congestion. Node Learning runs on edge devices, solving transmission bottlenecks while ensuring low latency, local data privacy, and robustness in weak networks."
        }
      },
      {
        "level": "intermediate",
        "question": {
          "zh": "Node Learning 与联邦学习（Federated Learning）有何异同？",
          "en": "How does Node Learning compare with Federated Learning (FL)?"
        },
        "answer": {
          "zh": "两者都保护隐私并减少数据传输。区别在于：联邦学习依赖中央服务器聚合模型，有单点瓶颈风险。Node Learning 完全去中心化，节点通过 P2P 方式交互知识，更具自适应性和可扩展性。",
          "en": "Both preserve privacy and reduce transmission. The difference: FL relies on a central server for model aggregation, creating a single point bottleneck. Node Learning is fully decentralized, using P2P interactions to share knowledge, making it more adaptive and scalable."
        }
      },
      {
        "level": "expert",
        "question": {
          "zh": "论文提到的“通过重叠和扩散传播”机制，对应哪些已有的分布式或共识理论？",
          "en": "What existing distributed or consensus theories correspond to the 'overlap and diffusion' mechanism?"
        },
        "answer": {
          "zh": "它关联了分布式共识、gossip 算法和分散式优化。知识的“扩散”类似 gossip 协议，“重叠”指节点间数据或任务的交集。该机制需融合这些理论，解决异构、异步和隐私条件下的模型收敛问题。",
          "en": "It relates to distributed consensus, gossip algorithms, and decentralized optimization. Knowledge 'diffusion' resembles gossip protocols, while 'overlap' means intersections in data or tasks among nodes. It integrates these theories to address convergence under heterogeneous, asynchronous, and private conditions."
        }
      }
    ]
  },
  {
    "date": "2026-02-21",
    "projects": [
      {
        "url": "https://arxiv.org/abs/2602.17223",
        "name": "Privacy-Preserving Mechanisms Enable Cheap Verifiable Inference of LLMs",
        "type": "论文",
        "tags": [
          "Arka Pal",
          "Verifiable Inference"
        ],
        "summary": {
          "zh": "Pal 等人在论文中提出了一种新见解，即利用隐私保护LLM推理方法，可以以极低的额外成本实现可验证推理。研究设计了两种新协议，通过添加少量额外计算令牌来提供推理过程保证，显著降低了验证开销。",
          "en": "Pal et al. proposes a novel insight that privacy-preserving LLM inference methods can enable verifiable inference at marginal extra cost. It designs two new protocols that add minimal computational tokens to provide guarantees, significantly reducing verification overhead."
        },
        "slogan": {
          "zh": "【AI安全】隐私保护新思路：用低成本实现大语言模型的可验证推理"
        },
        "notes": {
          "zh": [
            "核心洞察：隐私保护机制可低成本转化为可验证推理保证。",
            "问题背景：第三方托管LLM时，用户无法验证提供商是否运行了指定的大模型。",
            "现有方案（如ZKP）计算开销大，不适用于大型模型。",
            "提出两种新协议，仅需添加少量计算令牌即可验证推理。",
            "相比ZKP方法，新协议运行时间更短，对下游任务影响极小。",
            "建立了隐私性与可验证性在LLM推理中的新联系。"
          ],
          "en": [
            "Core insight: Privacy-preserving mechanisms can be repurposed for cheap verifiable inference.",
            "Problem: Users cannot verify if third-party providers run the specified large model.",
            "Existing solutions (e.g., ZKPs) are computationally expensive for large models.",
            "Two new protocols add minimal computational tokens for verification.",
            "Faster than ZKP methods with little downstream impact.",
            "Establishes a novel connection between privacy and verifiability in LLM inference."
          ]
        }
      }
    ],
    "qas": [
      {
        "level": "beginner",
        "question": {
          "zh": "论文主要解决LLM使用中的什么问题？",
          "en": "What main problem does this paper solve in LLM usage?"
        },
        "answer": {
          "zh": "解决第三方LLM托管服务的“偷梁换柱”问题：用户无法确认服务商是否真实运行了指定的大模型，还是用廉价小模型冒充。",
          "en": "Addresses the issue of dishonest third-party LLM providers secretly replacing specified expensive models with cheaper ones."
        }
      },
      {
        "level": "intermediate",
        "question": {
          "zh": "新协议如何操作？对推理影响多大？",
          "en": "How do the new protocols operate, and what is their impact on inference?"
        },
        "answer": {
          "zh": "核心是在隐私保护推理流程中，嵌入少量绑定模型身份的计算令牌。由于生成和验证成本极低，几乎不影响推理延迟和下游性能。",
          "en": "The core idea is embedding minimal, model-identity tokens into the privacy-preserving inference flow. It has almost no impact on latency or performance due to extremely low costs."
        }
      },
      {
        "level": "expert",
        "question": {
          "zh": "从密码学角度，如何评价将“隐私性”与“可验证性”结合的做法及其局限性？",
          "en": "Cryptographically, how to evaluate linking \"privacy\" and \"verifiability\", and what are the limitations?"
        },
        "answer": {
          "zh": "该做法基于MPC等隐私协议步骤可审计的特性来证明特定模型被执行。主要局限包括：1. 极度依赖底层隐私协议的安全性；2. 假设攻击者无法同时破坏隐私与验证机制；3. 需对特定模型结构进行编码以嵌入令牌，从而限制了通用性。",
          "en": "It uses the auditable nature of privacy protocols (like MPC) to prove model execution. Limitations: 1. Relies on underlying privacy security; 2. Assumes attackers cannot break both privacy and verification; 3. May require specific model encoding, limiting generality."
        }
      }
    ]
  }
]