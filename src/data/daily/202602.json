[
  {
    "date": "2026-02-05",
    "projects": [
      {
        "name": "Mosaic Learning: A Framework for Decentralized Learning with Model Fragmentation",
        "url": "https://arxiv.org/abs/2602.04352",
        "type": "论文",
        "tags": [
          "Sayan Biswas",
          "Fragmentation",
          "Non-IID",
          "Gossip"
        ],
        "summary": {
          "zh": "Biswas等人在{{name}}论文中提出Mosaic Learning框架，通过将模型参数分片并在不同通信图中独立传播，提升去中心化学习在非IID数据下的节点级性能，并给出理论与实验验证。",
          "en": "In the {{name}}paper, Biswas et al. introduced Mosaic Learning, a decentralized learning framework that uses model fragmentation across multiple gossip graphs to improve node-level performance, especially under non-IID data."
        },
        "slogan": {
          "zh": "【ML论文】模型不再整包发：Mosaic Learning 重塑去中心化训练"
        },
        "notes": {
          "zh": [
            "提出将model fragmentation作为去中心化学习的一等设计维度，而非系统层技巧",
            "不同模型分片通过独立gossip图传播，增强信息多样性并减少冗余同步",
            "理论证明最坏情况下收敛率不劣于Epidemic Learning（EL）",
            "凸问题中分片数量增加可降低系统收缩因子，加快一致性",
            "实验显示在强non-IID场景下，节点平均性能最高提升约12%",
            "发现consensus distance在非凸任务中不再是可靠的性能指标"
          ],
          "en": [
            "Treats model fragmentation as a first-class learning primitive in decentralized ML.",
            "Uses independent gossip graphs per fragment to improve information diversity.",
            "Matches state-of-the-art convergence guarantees of Epidemic Learning in theory.",
            "Shows faster consensus in convex settings as fragment count increases.",
            "Empirically boosts node-level accuracy under strong non-IID data.",
            "Finds consensus distance unreliable for non-convex performance evaluation."
          ]
        }
      }
    ],
    "qas": [
      {
        "level": "beginner",
        "question": {
          "zh": "Mosaic Learning 和传统的去中心化学习有什么直观区别？",
          "en": "How is Mosaic Learning intuitively different from standard decentralized learning?"
        },
        "answer": {
          "zh": "传统方法每轮交换完整模型，而 Mosaic Learning 将模型拆成多个子空间，每个子空间走不同的通信路径。这类似把一次“整体验证”拆成多次“分片验证”，让信息传播更分散、更高效。",
          "en": "Standard DL exchanges full models each round, while Mosaic Learning sends different parameter fragments over different paths. This spreads information more evenly, similar to splitting a monolithic check into multiple independent checks."
        }
      },
      {
        "level": "intermediate",
        "question": {
          "zh": "为什么模型分片不会破坏去中心化学习的收敛性？",
          "en": "Why does model fragmentation not hurt convergence guarantees?"
        },
        "answer": {
          "zh": "论文证明最坏情况下，每个分片的 gossip 在期望上等价于原始 EL 的通信算子。分片后分别分析再合并，可得到与 EL 相同的收敛率，且与分片数 K 无关，类似 batch evaluation 在总体上保持复杂度不变。",
          "en": "Each fragment's gossip operator matches Epidemic Learning in expectation. Analyzing fragments separately and aggregating results yields the same worst-case convergence rate as EL, independent of K, akin to batch evaluation preserving asymptotic cost."
        }
      },
      {
        "level": "expert",
        "question": {
          "zh": "从线性系统角度看，为什么增加分片数 K 会加快一致性？",
          "en": "From a linear system view, why does increasing K speed up consensus?"
        },
        "answer": {
          "zh": "分片使 gossip 矩阵呈 block-diagonal 结构，不同参数子空间独立混合。在线性二次模型中，这会降低系统收缩矩阵的最大特征值，减少共识误差，效果类似 sum-check 中分解高维多项式以加快验证。",
          "en": "Fragmentation yields a block-diagonal gossip structure where parameter subspaces mix independently. In quadratic models, this lowers the largest eigenvalue of the contraction matrix, reducing consensus error, similar to decomposing sums in sum-check protocols."
        }
      }
    ]
  },
  {
    "date": "2026-02-07",
    "projects": [
      {
        "name": "Privacy-Preserving LLM Inference in Practice: A Comparative Survey of Techniques, Trade-Offs, and Deployability",
        "url": "https://eprint.iacr.org/2026/105",
        "type": "论文",
        "tags": [
          "Davide Andreoletti",
          "Confidential Inference"
        ],
        "summary": {
          "zh": "Andreoletti等人在{{name}}论文中系统梳理了隐私保护LLM推理技术，从TEE、密码增强方案到FHE，对比其信任假设、可扩展性与部署成熟度，强调端到端仅客户端可见的强隐私模型。",
          "en": "In the {{name}}paper, Andreoletti et al. survey privacy-preserving LLM inference techniques, comparing TEE, crypto-augmented designs, and FHE in terms of trust, scalability, and deployment readiness under a strong end-to-end privacy model."
        },
        "slogan": {
          "zh": "【ZK/隐私计算】隐私版LLM如何落地？从TEE走向FHE的现实路线图"
        },
        "notes": {
          "zh": [
            "提出强隐私定义：Prompt与生成结果端到端仅客户端可见",
            "系统分析PETs在LLM推理中的应用，覆盖非线性层与自回归解码瓶颈",
            "TEE方案当前最具可部署性，但依赖硬件信任假设",
            "密码增强方案可降低对TEE的信任依赖，但计算成本显著上升",
            "FHE被视为长期理想方案，支持非交互式机密推理但尚不实用",
            "论文强调现实部署需在安全性、性能与信任最小化之间权衡"
          ],
          "en": [
            "Defines a strong privacy model where only clients can access prompts and outputs.",
            "Reviews PET-based approaches for LLM inference, including handling non-linear layers and autoregression.",
            "TEE-based systems are the most deployable today but rely on hardware trust.",
            "Crypto-augmented designs reduce hardware trust at higher computational cost.",
            "FHE is positioned as a principled long-term solution for confidential inference.",
            "Highlights trade-offs between security, performance, and deployability."
          ]
        }
      }
    ],
    "qas": [
      {
        "level": "beginner",
        "variant": 2,
        "question": {
          "zh": "为什么 Transformer 的非线性层对隐私推理特别困难？",
          "en": "Why are non-linear layers in Transformers especially challenging for private inference?"
        },
        "answer": {
          "zh": "非线性函数如 GELU、Softmax 难以在加密或受限执行环境中高效实现。它们往往需要近似、多轮交互或额外信任假设，成为隐私推理系统的主要性能瓶颈。",
          "en": "Non-linear functions such as GELU and Softmax are hard to implement efficiently under encryption or restricted execution. They often require approximations, interaction, or extra trust assumptions, making them major performance bottlenecks."
        }
      },
      {
        "level": "intermediate",
        "variant": 1,
        "question": {
          "zh": "TEE 方案为何被认为是当前最可部署的隐私推理路径？",
          "en": "Why are TEE-based solutions considered the most deployable today?"
        },
        "answer": {
          "zh": "TEE 能在几乎原生性能下运行完整模型，并支持自回归解码，工程复杂度较低。其主要代价是对硬件厂商和侧信道防护的信任，而非计算成本。",
          "en": "TEEs can run full models at near-native performance and support autoregressive decoding with low engineering complexity. The main cost is reliance on hardware trust and side-channel defenses rather than computation."
        }
      },
      {
        "level": "expert",
        "variant": 1,
        "question": {
          "zh": "自回归解码为何成为区分隐私推理方案可行性的关键因素？",
          "en": "Why is autoregressive decoding a key differentiator for private inference systems?"
        },
        "answer": {
          "zh": "自回归解码需要逐 token 依赖前序输出，这在交互式或高延迟加密方案中极其昂贵。能否高效支持解码，直接决定系统是否适用于真实 LLM 服务。",
          "en": "Autoregressive decoding requires token-by-token dependency, which is extremely costly under interactive or high-latency cryptographic schemes. Efficient decoding support largely determines real-world applicability."
        }
      }
    ]
  },
  {
    "date": "2026-02-08",
    "projects": [
      {
        "url": "https://arxiv.org/abs/2601.10823",
        "name": "Mugi: Value Level Parallelism For Efficient LLMs",
        "type": "论文",
        "tags": [
          "Daniel Price",
          "VLP",
          "GEMM"
        ],
        "summary": {
          "zh": "Price等人在{{name}}论文中系统研究了VLP在LLM中的应用，提出面向非线性与小批量场景的VLP优化，并设计新架构Mugi，在吞吐、能效与可持续性上显著优于现有方案。",
          "en": "In the {{name}}paper, Price et al. study VLP for LLMs, extending it to nonlinear ops and small-batch GEMMs, and propose the Mugi architecture to improve throughput, energy efficiency, and sustainability."
        },
        "slogan": {
          "zh": "【LLM系统】不只是GEMM：Mugi用VLP重塑高效与低碳的LLM推理"
        },
        "notes": {
          "zh": [
            "将VLP从对称GEMM推广到LLM中的非线性近似计算",
            "采用以值为中心的近似策略，为重要数值分配更高精度",
            "在端到端LLM精度与性能上优于现有非线性近似方法",
            "针对小batch、非对称输入的GEMM优化VLP实现",
            "结合权重量化、KV Cache量化与GQA等主流LLM优化",
            "Mugi架构在吞吐、能耗及运行与制造碳排放上均显著下降"
          ],
          "en": [
            "Extends VLP beyond symmetric GEMM to nonlinear LLM operations.",
            "Uses value-centric approximations to preserve important values.",
            "Achieves better end-to-end LLM accuracy and performance.",
            "Optimizes VLP for small-batch and asymmetric GEMMs.",
            "Integrates weight-only and KV cache quantization with GQA.",
            "Mugi improves throughput, energy efficiency, and reduces carbon footprint."
          ]
        }
      }
    ],
    "qas": [
      {
        "level": "beginner",
        "question": {
          "zh": "什么是 Value Level Parallelism（VLP），它最初解决什么问题？",
          "en": "What is Value Level Parallelism (VLP), and what problem was it originally designed to solve?"
        },
        "answer": {
          "zh": "VLP 是一种利用数值分布特性的并行计算方式，最初用于加速低精度、大批量的 GEMM 运算。它通过对不同“数值重要性”采用不同精度或计算路径，提高吞吐量与能效。",
          "en": "VLP is a parallelization technique that exploits value distributions. It was originally proposed to accelerate low-precision, large-batch GEMMs by assigning different accuracy or compute paths to values of different importance."
        }
      },
      {
        "level": "intermediate",
        "question": {
          "zh": "为什么小 batch、非对称 GEMM 对 VLP 是一项挑战？",
          "en": "Why are small-batch, asymmetric GEMMs challenging for VLP?"
        },
        "answer": {
          "zh": "传统 VLP 假设对称输入和大批量以摊销开销。LLM 推理中常见的小 batch、权重量化与 KV cache 量化破坏了这些假设，需要重新设计调度与数据路径。",
          "en": "Classic VLP assumes symmetric inputs and large batches to amortize overhead. LLM inference often uses small batches with weight-only and KV-cache quantization, breaking these assumptions and requiring new designs."
        }
      },
      {
        "level": "expert",
        "question": {
          "zh": "Mugi 架构如何同时支持多种 LLM 优化而不牺牲通用性？",
          "en": "How does the Mugi architecture support multiple LLM optimizations without sacrificing generality?"
        },
        "answer": {
          "zh": "Mugi 将 VLP 抽象为统一的值级执行框架，使权重量化、KV cache 量化和 GQA 成为可组合策略，而非硬编码路径，从而覆盖完整 Transformer 工作负载。",
          "en": "Mugi abstracts VLP into a unified value-level execution framework, making weight quantization, KV-cache quantization, and GQA composable strategies rather than fixed paths, enabling full Transformer support."
        }
      }
    ]
  },
  {
    "date": "2026-02-10",
    "projects": [
      {
        "name": "InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation",
        "url": "https://arxiv.org/abs/2602.08229",
        "type": "论文",
        "tags": [
          "Yifan Yang",
          "LLM Evaluation"
        ],
        "summary": {
          "zh": "作者在{{name}}论文中提出了一种基于区块链的去中心化LLM评估框架，通过激励全球贡献者作为独立验证者，利用异构计算节点进行大规模基准测试，以解决集中式评估的不透明、过拟合和硬件差异问题。",
          "en": "The authors propose a blockchain-based decentralized LLM evaluation framework in their {{name}} paper. It incentivizes global contributors as independent validators for large-scale benchmarking across heterogeneous compute nodes, addressing issues like opacity and hardware variance in centralized evaluations."
        },
        "notes": {
          "zh": [
            "集中式LLM评估不一致：HumanEval标准差(1.67)超过前10模型差距(0.91)。",
            "核心问题：评估不透明、模型过拟合、硬件差异导致波动。",
            "去中心化框架利用区块链激励全球参与者成为独立验证者。",
            "通过异构计算节点进行大规模基准测试，引入多样性。",
            "稳健奖励系统确保评估完整性并阻止不诚实参与。",
            "实验将标准差降至0.28，显著提升模型排名置信度。"
          ],
          "en": [
            "Centralized LLM evaluation shows high inconsistency: HumanEval std dev (1.67) exceeds top-10 model gap (0.91).",
            "Issues: opacity, overfitting, hardware-induced variance.",
            "Decentralized blockchain framework incentivizes global validators.",
            "Enables diversity via heterogeneous compute nodes.",
            "Robust reward system ensures integrity.",
            "Reduces std dev to 0.28, improving ranking confidence."
          ]
        }
      }
    ],
    "qas": [
      {
        "level": "beginner",
        "question": {
          "zh": "InfiCoEvalChain框架是如何利用区块链技术的？",
          "en": "How does the InfiCoEvalChain framework utilize blockchain technology?"
        },
        "answer": {
          "zh": "InfiCoEvalChain利用区块链激励全球参与者贡献计算资源成为独立验证节点，运行LLM评估任务。区块链确保评估记录不可篡改、透明，并通过共识机制保证结果完整性。",
          "en": "InfiCoEvalChain uses blockchain to incentivize global participants to contribute computing resources as independent validators for LLM evaluations. Blockchain ensures tamper-proof, transparent records and uses consensus to guarantee result integrity."
        }
      },
      {
        "level": "intermediate",
        "question": {
          "zh": "InfiCoEvalChain的\"去中心化背书\"机制相比传统评估有哪些优势？",
          "en": "What advantages does InfiCoEvalChain's decentralized endorsement offer over traditional evaluation?"
        },
        "answer": {
          "zh": "核心优势包括：1. **硬件多样性**：在全球异构节点上评估，减少单一环境偏差；2. **多方共识**：结果需多个独立验证者确认，提高可信度；3. **激励透明**：通过公开奖励系统激励诚实评估。",
          "en": "Key advantages: 1. **Hardware Diversity**: Evaluations on global heterogeneous nodes reduce single-environment bias; 2. **Multi-party Consensus**: Results require multiple independent validators, enhancing credibility; 3. **Transparent Incentives**: Public reward system incentivizes honest evaluation."
        }
      },
      {
        "level": "expert",
        "question": {
          "zh": "InfiCoEvalChain如何抵御女巫攻击或验证者合谋？",
          "en": "How does InfiCoEvalChain resist Sybil Attacks or validator collusion?"
        },
        "answer": {
          "zh": "对策包括：1. **女巫攻击**：结合质押机制或可信硬件验证身份；2. **验证者合谋**：随机分配任务、盲评估、Slashing惩罚使合谋收益为负；3. **奖励设计**：引入信誉系统区分诚实与恶意行为。",
          "en": "Countermeasures: 1. **Sybil Attacks**: Staking mechanisms or trusted hardware for identity; 2. **Collusion**: Random task assignment, blind evaluation, slashing penalties; 3. **Incentive Design**: Reputation systems to distinguish honest from malicious behavior."
        }
      }
    ]
  }
]