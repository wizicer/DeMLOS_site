[
  {
    "date": "2026-02-05",
    "projects": [
      {
        "name": "Mosaic Learning: A Framework for Decentralized Learning with Model Fragmentation",
        "url": "https://arxiv.org/abs/2602.04352",
        "type": "论文",
        "tags": [
          "Sayan Biswas",
          "Fragmentation",
          "Non-IID",
          "Gossip"
        ],
        "summary": {
          "zh": "Biswas等人在{{name}}论文中提出Mosaic Learning框架，通过将模型参数分片并在不同通信图中独立传播，提升去中心化学习在非IID数据下的节点级性能，并给出理论与实验验证。",
          "en": "In the {{name}}paper, Biswas et al. introduced Mosaic Learning, a decentralized learning framework that uses model fragmentation across multiple gossip graphs to improve node-level performance, especially under non-IID data."
        },
        "slogan": {
          "zh": "【ML论文】模型不再整包发：Mosaic Learning 重塑去中心化训练"
        },
        "notes": {
          "zh": [
            "提出将model fragmentation作为去中心化学习的一等设计维度，而非系统层技巧",
            "不同模型分片通过独立gossip图传播，增强信息多样性并减少冗余同步",
            "理论证明最坏情况下收敛率不劣于Epidemic Learning（EL）",
            "凸问题中分片数量增加可降低系统收缩因子，加快一致性",
            "实验显示在强non-IID场景下，节点平均性能最高提升约12%",
            "发现consensus distance在非凸任务中不再是可靠的性能指标"
          ],
          "en": [
            "Treats model fragmentation as a first-class learning primitive in decentralized ML.",
            "Uses independent gossip graphs per fragment to improve information diversity.",
            "Matches state-of-the-art convergence guarantees of Epidemic Learning in theory.",
            "Shows faster consensus in convex settings as fragment count increases.",
            "Empirically boosts node-level accuracy under strong non-IID data.",
            "Finds consensus distance unreliable for non-convex performance evaluation."
          ]
        }
      }
    ],
    "qas": [
      {
        "level": "beginner",
        "question": {
          "zh": "Mosaic Learning 和传统的去中心化学习有什么直观区别？",
          "en": "How is Mosaic Learning intuitively different from standard decentralized learning?"
        },
        "answer": {
          "zh": "传统方法每轮交换完整模型，而 Mosaic Learning 将模型拆成多个子空间，每个子空间走不同的通信路径。这类似把一次“整体验证”拆成多次“分片验证”，让信息传播更分散、更高效。",
          "en": "Standard DL exchanges full models each round, while Mosaic Learning sends different parameter fragments over different paths. This spreads information more evenly, similar to splitting a monolithic check into multiple independent checks."
        }
      },
      {
        "level": "intermediate",
        "question": {
          "zh": "为什么模型分片不会破坏去中心化学习的收敛性？",
          "en": "Why does model fragmentation not hurt convergence guarantees?"
        },
        "answer": {
          "zh": "论文证明最坏情况下，每个分片的 gossip 在期望上等价于原始 EL 的通信算子。分片后分别分析再合并，可得到与 EL 相同的收敛率，且与分片数 K 无关，类似 batch evaluation 在总体上保持复杂度不变。",
          "en": "Each fragment's gossip operator matches Epidemic Learning in expectation. Analyzing fragments separately and aggregating results yields the same worst-case convergence rate as EL, independent of K, akin to batch evaluation preserving asymptotic cost."
        }
      },
      {
        "level": "expert",
        "question": {
          "zh": "从线性系统角度看，为什么增加分片数 K 会加快一致性？",
          "en": "From a linear system view, why does increasing K speed up consensus?"
        },
        "answer": {
          "zh": "分片使 gossip 矩阵呈 block-diagonal 结构，不同参数子空间独立混合。在线性二次模型中，这会降低系统收缩矩阵的最大特征值，减少共识误差，效果类似 sum-check 中分解高维多项式以加快验证。",
          "en": "Fragmentation yields a block-diagonal gossip structure where parameter subspaces mix independently. In quadratic models, this lowers the largest eigenvalue of the contraction matrix, reducing consensus error, similar to decomposing sums in sum-check protocols."
        }
      }
    ]
  },
  {
    "date": "2026-02-07",
    "projects": [
      {
        "name": "Privacy-Preserving LLM Inference in Practice: A Comparative Survey of Techniques, Trade-Offs, and Deployability",
        "url": "https://eprint.iacr.org/2026/105",
        "type": "论文",
        "tags": [
          "Davide Andreoletti",
          "Confidential Inference"
        ],
        "summary": {
          "zh": "Andreoletti等人在{{name}}论文中系统梳理了隐私保护LLM推理技术，从TEE、密码增强方案到FHE，对比其信任假设、可扩展性与部署成熟度，强调端到端仅客户端可见的强隐私模型。",
          "en": "In the {{name}}paper, Andreoletti et al. survey privacy-preserving LLM inference techniques, comparing TEE, crypto-augmented designs, and FHE in terms of trust, scalability, and deployment readiness under a strong end-to-end privacy model."
        },
        "slogan": {
          "zh": "【ZK/隐私计算】隐私版LLM如何落地？从TEE走向FHE的现实路线图"
        },
        "notes": {
          "zh": [
            "提出强隐私定义：Prompt与生成结果端到端仅客户端可见",
            "系统分析PETs在LLM推理中的应用，覆盖非线性层与自回归解码瓶颈",
            "TEE方案当前最具可部署性，但依赖硬件信任假设",
            "密码增强方案可降低对TEE的信任依赖，但计算成本显著上升",
            "FHE被视为长期理想方案，支持非交互式机密推理但尚不实用",
            "论文强调现实部署需在安全性、性能与信任最小化之间权衡"
          ],
          "en": [
            "Defines a strong privacy model where only clients can access prompts and outputs.",
            "Reviews PET-based approaches for LLM inference, including handling non-linear layers and autoregression.",
            "TEE-based systems are the most deployable today but rely on hardware trust.",
            "Crypto-augmented designs reduce hardware trust at higher computational cost.",
            "FHE is positioned as a principled long-term solution for confidential inference.",
            "Highlights trade-offs between security, performance, and deployability."
          ]
        }
      }
    ],
    "qas": [
      {
        "level": "beginner",
        "variant": 2,
        "question": {
          "zh": "为什么 Transformer 的非线性层对隐私推理特别困难？",
          "en": "Why are non-linear layers in Transformers especially challenging for private inference?"
        },
        "answer": {
          "zh": "非线性函数如 GELU、Softmax 难以在加密或受限执行环境中高效实现。它们往往需要近似、多轮交互或额外信任假设，成为隐私推理系统的主要性能瓶颈。",
          "en": "Non-linear functions such as GELU and Softmax are hard to implement efficiently under encryption or restricted execution. They often require approximations, interaction, or extra trust assumptions, making them major performance bottlenecks."
        }
      },
      {
        "level": "intermediate",
        "variant": 1,
        "question": {
          "zh": "TEE 方案为何被认为是当前最可部署的隐私推理路径？",
          "en": "Why are TEE-based solutions considered the most deployable today?"
        },
        "answer": {
          "zh": "TEE 能在几乎原生性能下运行完整模型，并支持自回归解码，工程复杂度较低。其主要代价是对硬件厂商和侧信道防护的信任，而非计算成本。",
          "en": "TEEs can run full models at near-native performance and support autoregressive decoding with low engineering complexity. The main cost is reliance on hardware trust and side-channel defenses rather than computation."
        }
      },
      {
        "level": "expert",
        "variant": 1,
        "question": {
          "zh": "自回归解码为何成为区分隐私推理方案可行性的关键因素？",
          "en": "Why is autoregressive decoding a key differentiator for private inference systems?"
        },
        "answer": {
          "zh": "自回归解码需要逐 token 依赖前序输出，这在交互式或高延迟加密方案中极其昂贵。能否高效支持解码，直接决定系统是否适用于真实 LLM 服务。",
          "en": "Autoregressive decoding requires token-by-token dependency, which is extremely costly under interactive or high-latency cryptographic schemes. Efficient decoding support largely determines real-world applicability."
        }
      }
    ]
  }
]